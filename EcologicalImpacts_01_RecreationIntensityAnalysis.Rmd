---
title: "DrivingExtractions"
author: "Madeline Aberg"
date: "10/21/2021"
output: pdf_document
---

Last updated: July 25, 2022

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Resources
https://isdrfall21.classes.spaseslab.com/content/13-content/ 
https://isdrfall21.classes.spaseslab.com/slides/13-slides.html#12
https://mgimond.github.io/Spatial/chp11_0.html
https://oliviergimenez.github.io/intro_spatialR/#74
https://www.maths.usyd.edu.au/u/UG/SM/STAT3022/r/current/Misc/data-visualization-2.1.pdf
https://egallic.fr/R/sKDE/smooth-maps/kde.html
https://andrewpwheeler.com/tag/kernel-density/
https://www.jratcliffe.net/copy-of-aoristic-analysis


# Purpose
This RMarkdown covers the entire process needed to extract recreation intensity values for paired sites or nests. 

## Driving Data
We used 10 driving routes spread across the NCA, each 16 km in length, that passed through a variety of land cover types and utilized publicly accessible major and minor roads. From April 2019 to July 2021, we drove the routes each Saturday during the recreational shooting season (March - July) and on one randomly selected Saturday per month throughout the rest of the year. During the routes, we recorded the location, number of people, demographics, and number of vehicles for each observation of human use. We classified the type of use as motorized recreation (off-highway vehicles, driving off-road), target shooting (shooting at targets or other inanimate objects, stationary), hunting (moving through vegetation and shooting, no targets), non-motorized recreation (hiking, birdwatching, biking), and other (camping, photography, etc.).  
  
*Note: I'm using all types of recreation for recreation intensity (not only shooting),. I also included parked cars assumed to be recreational users (e.g., had an ohv trailer), but we could easily sort out observations where the number of people = 0*

## 1km2 Sites
Within the NCA, we selected 10 paired sites in locations with varying amounts of recreational use. Each site was 1 km2 and was paired with a site with similar vegetative composition. We also designated 2 larger areas for nest searching and monitoring. We compared land cover, elevation, and road density between the paired sites to ensure that one group was not biased by another variable.

1km2 sites are used to extract recreation values for ground squirrel abundance, raptor and raven abundance, mammalian scavenger abundance, and ground-nesting bird (HOLA/LBCU) abundance 

## Horned Lark & Long-billed Curlew Nests
Within the designated nest searching areas, we searched for horned lark nests (March 27 to July 16, 2020, and March 15 to July 18, 2021) and long-billed curlew nests (May 2018 to June 2018, April to June 2019, 2020, 2021). We found nests using behavioral cues, including courtship, nest building, feeding behaviors, and incidentally by flushing birds off nests while walking systematically through promising habitat. During the initial visit, we recorded the location of the nest, measured the developmental stage of the incubating eggs or chicks, and calculated a predicted hatch and fledge date. After the initial visit, we checked nests every 2 to 3 days from a distance to minimize disturbance. As the predicted hatching or fledging date approached, we checked nests daily to better determine nest fate. Within 7 days of the fledge (horned lark) or hatch (curlew) date, we conducted a habitat survey of the nest site. At each nest site, we surveyed the nest and collected data on habitat variables of interest.  
  
Nest locations were used to extract recreation values for nest success analyses for the weeks the nest was active. 

# Workflow
1) Load packages & functions.  
2) Load spatial & tabular data.  
3) Set CRS for spatial data - Lat/Long WGS 84 & NAD83 UTM Zone 11.  
4) Join driving points & tabular data, filter by recreation type.  
5) For loop - filter by time period, adjust point location, make (& save) a kernel density raster, extract recreation intensity for a given area (paired site, nests), and save extraction values in a dataframe.  
  
*Note: Steps 1-4 are the same no matter what sites you're extracting for. The for loop in Step 5 will need to be adjusted depending on the area you're extracting for (paired site or nest), buffer size, and date.*

# Load packages & functions
## Packages
This code block loads all of the necessary packages for cleaning & plotting the data. 
```{r}
library(dplyr)
library(tidyverse)
library(ggplot2)
library(viridis)
library(sf)
```

## Functions
This code block loads the functions that will be used in Step 5 (*described below*). 
```{r}
source("Functions.R", local = knitr::knit_global())
```

*PointAdjust(df): adjusts points based on the recorded distance (m) and bearing* 
-df needs to be sf object, CRS = LatLong, WGS84

*KD_extractsave(W, P, d, E, b): creates and saves a kernel density raster using the adjusted points, extracts the raster values*
- W = window or boundary for the extraction (sf object, NAD83/UTM zone 11N)
- P = driving observation points (Spatial Points, Lat/Long WGS84) * should be adjusted points from PointAdjust()
- d = distance in meters for the sigma of the kernel density (have been using 1000)
- E = points to use in the raster extraction (paired sites, nests) - sf object, NAD83 UTM zone 11
- b = buffer size for raster extraction in meters

# Load spatial & tabular data 
## Spatial Data
```{r}
# Horned Lark nests for 2020 & 2021
HolaNests_ll <- read_sf(here::here("datatemp/original/HOLA/HOLA/Nests/Combined_Nests.shp")) # sf, CRS: Lat/Long WGS 84
HolaNests_ll <- HolaNests_ll[-121,] # one duplicate nest (PV-8)


# Horned Lark Nest Search Area for 2021
HolaNSA_ll <- read_sf(here::here("datatemp/original/HOLA/HOLA/NestSearchArea/2021_NestSearchAreas/2021_NestSearchAreas.shp")) # sf, CRS: Lat/Long WGS 84

# LBCU nests for 2019 - 2021
LbcuNests <- read.csv(here::here("datatemp/original/LBCUNests.csv"))
LbcuNests_utm <- st_as_sf(LbcuNests, coords = c("UTMx_nest", "UTMy_nest"), crs = 26911)
LbcuNests_ll <- st_as_sf(LbcuNests, coords = c("UTMx_nest", "UTMy_nest"), crs = 4326)

# 10 x 1km^2 plots at the NCA used in 2021
PairedSites_ll <- read_sf(here::here("datatemp/original/HOLA/HOLA/2021_PairedSites/PairedSites2021/PairedSites2021.shp")) # sf, CRS: Lat/Long WGS 84
PairedSites <- PairedSites_ll$SiteID # Vector of Paired Site labels in order of sf

# driving routes used to survey for recreational use
routes_utm <- read_sf(here::here("datatemp/original/2019drivingroutes/2019DrivingRoutes.shp")) #sf, CRS: NAD83 UTM zone 11N

# NCA boundary
NCA_utm <- read_sf(here::here("datatemp/original/NCA_boundary/NCA_boundary.shp")) #sf, CRS: NAD83 UTM zone 11N

# NCA boundary PLUS - includes PV1 area/nests
NCAplus_utm <- read_sf("datatemp/original/NCA_boundary_plus/NCA_boundary_plus.shp") #sf, CRS: NAD83 UTM zone 11N

# Driving observations 
driving_pts_ll <- read_sf(here::here("datatemp/original/AllDrivingPts/AllDrivingPts_Final.shp")) # sf, CRS: Lat/Long WGS 84
```

## Tabular data
Run the R Script that loads & cleans the tabular, observation-level data. 
full code listed below

```{r}
source("EcologicalImpacts_01a_CleanDriveTabData.R", local = knitr::knit_global())


```

# Set CRS for spatial data - Lat/Long WGS 84 & NAD83 UTM Zone 11.  
```{r}
driving_pts_utm <- st_transform(driving_pts_ll, crs=st_crs(NCA_utm)) # re-projection command -> UTM Z11 NAD83
HolaNests_utm <- st_transform(HolaNests_ll, crs = st_crs(NCA_utm)) # re-project -> NAD83 UTM zone 11N
HolaNSA_utm <- st_transform(HolaNSA_ll, crs = st_crs(NCA_utm)) # re-project -> NAD83 UTM zone 11N
PairedSites_utm <- st_transform(PairedSites_ll, crs=st_crs(NCA_utm)) # re-project -> NAD83 UTM zone 11N
```

# Join driving points & tabular data, filter by recreation type.  
*Need the points to be in Lat/Long WGS 84 to start the process*
```{r}
driving_all_ll <- driving_pts_ll %>% 
  left_join(., tab_data, by = c("name" = "point.id"))%>%
  filter(., CHECK. == "YES")

write.csv(driving_all_ll, "C:/Users/Madeline/Desktop/EcologicalImpactsManuscript/CleanData/DrivingPts_Clean_19Aug2024.csv")

#### Make subsets of the data ####
rec.ll <- filter(driving_all_ll, use.type == "bike and run" | use.type == "biking" | use.type == "birding" | use.type == "camp"
                 | use.type == "camp and ohv" | use.type == "camp, ohv, kayak"| use.type == "camping"| use.type == "camping/ohv"
                 | use.type == "collecting brass"| use.type == "driving off road"| use.type == "hiking"| use.type == "hiking/camping"
                 | use.type == "horse riding"| use.type == "hunt and ohv" |use.type == "hunt and target"|use.type == "off road driving"
                 | use.type == "ohv"| use.type == "ohv + target"| use.type == "ohv and camping" | use.type == "ohv and hunt"
                 | use.type == "ohv and target"| use.type == "other - flying drone"| use.type == "other - picking up trash"
                 | use.type == "clean up" | use.type == "photography"| use.type == "picking up shells"| use.type == "picking up trash"
                 | use.type == "playing music"| use.type == "road bike"| use.type == "road biking"| use.type == "shooting"
                 | use.type == "shooting-clay pigeons"| use.type == "shooting-garbage"| use.type == "shooting-grsq"
                 | use.type == "shooting-target"| use.type == "sitting"| use.type == "sitting at overlook"
                 | use.type == "sitting in car with binoculars"
                 | use.type == "sitting in jeep"| use.type == "sitting on truck"| use.type == "target + photography"
                 | use.type == "target and photography"| use.type == "target shoot & collecting brass"
                 | use.type == "target shooting and hunting"
                 | use.type == "target shooting and walking"| use.type == "training/shooting"| use.type == "walking" 
                 | use.type == "walking dog"
                 | use.type == "wildlife photography"| use.type == "militia")

shoot.ll <- filter(driving_all_ll, NG.use.code == "hunting" | NG.use.code == "target shooting")

hunt.ll <- filter(driving_all_ll, NG.use.code == "hunting")

target.ll <- filter(driving_all_ll, NG.use.code == "target shooting")

ohv.11 <- filter(driving_all_ll, NG.use.code == "motorized recreation")

otherrec.11 <- filter(rec.ll, NG.use.code == "non-motorized recreation" | NG.use.code == "other")
```

# Plot # obs/route for each survey for highest (PV1) & lowest (BB2) route (Figure 3 in manuscript)
```{r}
PV1.rec <- filter(rec.ll, route == "PV1")
BB2.rec <- filter(rec.ll, route == "BB2")

PV1.rec$Obs <- 1

PV1.sum <- PV1.rec %>%
  group_by(date) %>%
  summarize(NumObs = sum(Obs))

PV1.stats <- PV1.sum %>%
  summarise(
          MeanObs = mean (NumObs), 
          SEObs = sd(NumObs),
          MinObs = min(NumObs), 
          MaxObs = max(NumObs))

PV1.sum$SurveyDay <- 1:nrow(PV1.sum)

library(ggplot2)
high <- ggplot(data = PV1.sum, aes(SurveyDay, NumObs))+
  geom_col() +
  labs(title = "Highest use route (A)", x = "Survey day", y = "Recreation observations") + 
  theme_classic()+
  theme(text=element_text(family="Times New Roman", size=20))

BB2.rec$Obs <- 1

date.all <- PV1.sum$date

BB2.sum <- BB2.rec %>%
  group_by(date) %>%
  summarize(NumObs = sum(Obs))


b.df <- as.data.frame(date.all)
b.df <- b.df %>%
  mutate(Obs = case_when(
    date.all == "2020-01-25" ~ 1, 
    date.all == "2020-02-22" ~ 1,
    date.all == "2020-07-11" ~ 1,
    date.all == "2020-11-21" ~ 1,
    date.all == "2021-02-06" ~ 1,
    date.all == "2021-03-27" ~ 2,
    date.all == "2021-05-01" ~ 1,
    date.all == "2021-05-29" ~ 1))

b.df$Obs <- b.df$Obs %>% replace(is.na(.), 0)
b.df$SurveyDay <- 1:nrow(b.df)

BB2.stats <- b.df %>%
  summarise(
          MeanObs = mean (Obs), 
          SEObs = sd(Obs),
          MinObs = min(Obs), 
          MaxObs = max(Obs))

library(ggplot2)
low <- ggplot(data = b.df, aes(SurveyDay, Obs, ymax = 35))+
  geom_col() +
  scale_y_continuous(limits = c(0, 32))+
  labs(title = "Lowest use route (B)", x = "Survey day", y = "Recreation observations") + 
  theme_classic()+
  theme(text=element_text(family="Times New Roman", size=20))


library(gridExtra)
grid.arrange(high, low, ncol = 2)
```

# Calculate % shooting for paper
```{r}
table(rec.ll$NG.use.code)

200 + 898 # shooting

200 + 149 + 138 + 149 + 898 # all

1098/1534

149/1534
(138 +149)/1534

```

# Extraction Process
For loop - filter by time period, adjust point location, make a kernel density raster, extract recreation intensity for a given area (paired site, nests), and save extraction values in a dataframe. 

If using the entire time frame, don't need to do a for loop, just use the functions.

## Shooting Points for the duration of the study  
Currently running with a 2 km buffer surrounding the paired sites
```{r}
shoot.adj <-  PointAdjust(shoot.ll)
shoot.extract <-KD_extractsave(NCA_utm, shoot.adj, 1000, PairedSites_utm, 2000)
shoot.results <- data.frame(row.names = PairedSites)
shoot.results$all <- shoot.extract
shoot.results
```

## Paired sites recreation intensity - comparing buffer size & results - *For analyses* (11/19/2021) + other rec groups (2/16/22)
Using all recreation, each week of the study & 2000m buffer for 2021 study period (March - July).   
Trying different buffer sizes - 500m, 1000m, 2000m, 5000m.  
This is what is used in the analyses at the paired site level (ground squirrels, raptors & ravens, mammalian scavengers, HOLA/LBCU abundance).
```{r}
PairedSites_BufferComp <- data.frame(row.names = PairedSites) # empty dataframe to save results

# for the entire duration of the study period, with different buffer sizes
for(i in 1:10) {
  
  x_adj <- PointAdjust(rec.ll) # Adjust all recreation observation points
  
  x_KD <- KD_extract(NCAplus_utm, x_adj, 1000, PairedSites_utm, 500*i) # extract w buffer size, increases by 500m
  
  PairedSites_BufferComp$x <- x_KD
  
  colnames(PairedSites_BufferComp)[i] <- paste0("WholeBuffer", i*500)
}

#### For March to July 2021 ####
a <- rec.ll %>% filter(date > "2021-03-01") # filter just for the 2021 season
a_adj <- PointAdjust(a) # adjust filtered points
a_KD <- KD_extract(NCAplus_utm, a_adj, 1000, PairedSites_utm, 2000) # extract with 2000 m buffer
PairedSites_BufferComp$MarJul2021_Buffer2km <- a_KD # save extracted values in the dataframe

kableExtra::kable(PairedSites_BufferComp)


#### Looking at specific types of recreation for March to July 2021 ####
# SHOOTING #
b <- shoot.ll %>% filter(date > "2021-03-01")
b_adj <- PointAdjust(b)
b_KD <- KD_extract(NCAplus_utm, b_adj, 1000, PairedSites_utm, 2000)
PairedSites_BufferComp$MarJul2021_Shoot <- b_KD

# OHV #
c <- ohv.11 %>% filter(date > "2021-03-01")
c_adj <- PointAdjust(c)
c_KD <- KD_extract(NCAplus_utm, c_adj, 1000, PairedSites_utm, 2000)
PairedSites_BufferComp$MarJul2021_OHV <- c_KD

# other rec # 
d <- otherrec.11 %>% filter(date > "2021-03-01")
d_adj <- PointAdjust(d)
d_KD <- KD_extract(NCAplus_utm, d_adj, 1000, PairedSites_utm, 2000)
PairedSites_BufferComp$MarJul2021_other <- d_KD

#### Save csv ####
write.csv(PairedSites_BufferComp, "PairedRecIntensity.csv") # this is the file that can be taken & used in HOLA analyses - saving in Driving Analysis folder

write.csv(PairedSites_BufferComp, file = "/Users/madelineaberg/Desktop/HOLA2020/2020analysis/HOLA_analysis/datatemp/original/PairedRecIntensity.csv") # save in HOLA analysis R project



### trying to extract for full NCA ####
# all rec, full study 
x_adj <- PointAdjust(rec.ll)
NCA_x <- KD_extract(NCAplus_utm, x_adj, 1000, NCAplus_utm, 0)
mean(NCA_x)

# all, Mar - July 2021
NCA_a <- KD_extract(NCAplus_utm, a_adj, 1000, NCAplus_utm, 0)
mean(NCA_a)

# shoot, Mar - Jul 2021
NCA_b <- KD_extract(NCAplus_utm, b_adj, 1000, NCAplus_utm, 0)
NCA_b
mean(NCA_b)

# ohv mar - jul 2021
NCA_c <- KD_extract(NCAplus_utm, c_adj, 1000, NCAplus_utm, 0)
mean(NCA_c)

# other rec mar - jul 2021
NCA_d <- KD_extract(NCAplus_utm, d_adj, 1000, NCAplus_utm, 0)
mean(NCA_d)

```

Buffer size doesn't make a difference. Checked this by manually changing the buffer size, and the extraction values did not change. So based on that, we should be able to use any buffer size.
  
Filtering by time period *does* make a difference (*makes sense, there are less points, so lower intensity*). It does show a slightly different pattern between sites though. 

## HOLA nests for analyses
Using all recreation for the duration of the field season (March - July 2020 & March - July 2021)
Buffer sizes - 10m, 500m, 1000m, full NSA  
*Feb 2022 - added 100m buffer, using for analysis*
```{r}
# Dataframe 
HOLArec <- data.frame(row.names = HolaNests_utm$WhichNest)

# Filter rec points by date
MarJul2021 <- rec.ll %>% filter(date > "2021-03-01") # filter just for the 2021 season
MarJul2020 <- rec.ll %>% filter(date > "2020-03-01" & date < "2020-08-01") # just 2020

#### Compare buffer sizes ####
a_adj <- PointAdjust(MarJul2020) # adjust filtered points
b_adj <- PointAdjust(MarJul2021)
# 10 m (territory)
a_KD <- KD_extract(NCAplus_utm, a_adj, 1000, HolaNests_utm, 10)
b_KD <- KD_extract(NCAplus_utm, b_adj, 1000, HolaNests_utm, 10)
HOLArec$Buffer10m_2020 <- a_KD
HOLArec$Buffer10m_2021 <- b_KD
# 100 m (territory +)
a_KD <- KD_extract(NCAplus_utm, a_adj, 1000, HolaNests_utm, 100)
b_KD <- KD_extract(NCAplus_utm, b_adj, 1000, HolaNests_utm, 100)
HOLArec$Buffer100m_2020 <- a_KD
HOLArec$Buffer100m_2021 <- b_KD
# 500 m 
a_KD <- KD_extract(NCAplus_utm, a_adj, 1000, HolaNests_utm, 500)
b_KD <- KD_extract(NCAplus_utm, b_adj, 1000, HolaNests_utm, 500)
HOLArec$Buffer500m_2020 <- a_KD
HOLArec$Buffer500m_2021 <- b_KD
# 1000 m
a_KD <- KD_extract(NCAplus_utm, a_adj, 1000, HolaNests_utm, 1000)
b_KD <- KD_extract(NCAplus_utm, b_adj, 1000, HolaNests_utm, 1000)
HOLArec$Buffer1000m_2020 <- a_KD
HOLArec$Buffer1000m_2021 <- b_KD

# look at values
kableExtra::kable(HOLArec)

 # this is the file that can be taken & used in HOLA analyses 
write.csv(HOLArec, file = "/Users/madelineaberg/Desktop/MadelineAbergBSUDissertation/03_CSVData/HOLArec.csv") # saved in HOLA analysis R project
```


## LBCU Nests for analyses (Feb 2022)
```{r}
# Dataframe 
LBCUrec <- data.frame(row.names = LbcuNests_utm$Nest.Name)

# Filter rec points by date
MarJun2021 <- rec.ll %>% filter(date > "2021-03-01" & date < "2021-07-01") # filter just for the 2021 season
MarJun2020 <- rec.ll %>% filter(date > "2020-03-01" & date < "2020-07-01") # just 2020
MarJun2019 <- rec.ll %>% filter(date < "2019-07-01")

#### Compare buffer sizes ####
a_adj <- PointAdjust(MarJun2019) # adjust filtered points
b_adj <- PointAdjust(MarJun2020)
c_adj <- PointAdjust(MarJun2021)

# 10 m 
a_KD <- KD_extract(NCAplus_utm, a_adj, 1000, LbcuNests_utm, 10)
b_KD <- KD_extract(NCAplus_utm, b_adj, 1000, LbcuNests_utm, 10)
c_KD <- KD_extract(NCAplus_utm, c_adj, 1000, LbcuNests_utm, 10)
LBCUrec$Buffer10m_2019 <- a_KD
LBCUrec$Buffer10m_2020 <- b_KD
LBCUrec$Buffer10m_2021 <- c_KD

# 100 m 
a_KD <- KD_extract(NCAplus_utm, a_adj, 1000, LbcuNests_utm, 100)
b_KD <- KD_extract(NCAplus_utm, b_adj, 1000, LbcuNests_utm, 100)
c_KD <- KD_extract(NCAplus_utm, x_adj, 1000, LbcuNests_utm, 100)

LBCUrec$Buffer100m_2019 <- a_KD
LBCUrec$Buffer100m_2020 <- b_KD
LBCUrec$Buffer100m_2021 <- c_KD

# 500 m (territory)
a_KD <- KD_extract(NCAplus_utm, a_adj, 1000, LbcuNests_utm, 500)
b_KD <- KD_extract(NCAplus_utm, b_adj, 1000, LbcuNests_utm, 500)
c_KD <- KD_extract(NCAplus_utm, c_adj, 1000, LbcuNests_utm, 500)

LBCUrec$Buffer500m_2019 <- a_KD
LBCUrec$Buffer500m_2020 <- b_KD
LBCUrec$Buffer500m_2021 <- c_KD

# 1000 m
a_KD <- KD_extract(NCAplus_utm, a_adj, 1000, LbcuNests_utm, 1000)
b_KD <- KD_extract(NCAplus_utm, b_adj, 1000, LbcuNests_utm, 1000)
c_KD <- KD_extract(NCAplus_utm, c_adj, 1000, LbcuNests_utm, 1000)

LBCUrec$Buffer1000m_2019 <- a_KD
LBCUrec$Buffer1000m_2020 <- b_KD
LBCUrec$Buffer1000m_2021 <- c_KD

# look at values
kableExtra::kable(LBCUrec)

write.csv(LBCUrec, "LBCUrec.csv") # this is the file that can be taken & used in HOLA analyses - saving in Driving Analysis folder
write.csv(LBCUrec, file = "/Users/madelineaberg/Desktop/HOLA2020/2020analysis/HOLA_analysis/datatemp/original/LBCUrec.csv") # save in HOLA analysis R project

```



## Trying for loop by time periods 
### by year - all recreation, NCAplus (working 11/01/2021)
```{r}
AllRec_Years <- data.frame(row.names = PairedSites)

for (i in 1:3) {
  x <- rec.ll %>% filter(year.num == i) # filter for each year (2019 = 1, 2020 = 2, 2021 = 3)
  
  x_adj <- PointAdjust(x) # Go through the PointAdjust function with points from year i
  
  x_KD <- KD_extract(NCAplus_utm, x_adj, 1000, PairedSites_utm, 2000) # kernel density with adjusted points
   
  AllRec_Years$y <- x_KD # add extraction values to the dataframe
  
  colnames(AllRec_Years)[i] <- paste0("Year",i) # rename column 
}

AllRec_Years
```
Lower recreational use in 2021 in SF1, PV1, PV2, CC2, BB2 (barely), PV3.  
Higher use in CC1, BB1, BB3, SF2

### by month - all recreation, NCAplus (working 11/01/2021)
this groups observations from all years for a month 
```{r}
AllRec_Months <- data.frame(row.names = PairedSites)

for (i in 1:12) {
  x <- rec.ll %>% filter(month.num == i) # filter for each month (2019 = 1, 2020 = 2, 2021 = 3)
  
  x_adj <- PointAdjust(x) # Go through the PointAdjust function with points from month i
  
  x_KD <- KD_extract(NCAplus_utm, x_adj, 1000, PairedSites_utm, 2000) # kernel density with adjusted points
   
  AllRec_Months$y <- x_KD # add extraction values to the dataframe
  
  colnames(AllRec_Months)[i] <- paste0("Month",i) # rename column 
}

AllRec_Months
```

# HOLA Nests
## study duration with HOLA nests - testing buffer size
want to test 50m, 1000m, entire NSA as buffer sizes 
```{r}
HOLA_BufferComp <- data.frame(row.names = HolaNests_ll$WhichNest) # empty dataframe to save results

# for the entire duration of the study period, with different buffer sizes
for(i in 1:10) {
  
  x_adj <- PointAdjust(rec.ll) # Adjust all recreation observation points
  
  x_KD <- KD_extract(NCAplus_utm, x_adj, 1000, PairedSites_utm, 500*i) # extract w buffer size, increases by 500m
  
  HOLA_BufferComp$x <- x_KD
  
  colnames(HOLA_BufferComp)[i] <- paste0("WholeBuffer", i*500)
}
```

## HOLA nests for analyses
For now, using all recreation for the duration of the field season (March - July 2020 & March - July 2021)
Buffer sizes - 10m, 500m, 1000m, full NSA
```{r}
# Dataframe 
HOLArec <- data.frame(row.names = HolaNests_utm$WhichNest)

# Filter rec points by date
MarJul2021 <- rec.ll %>% filter(date > "2021-03-01") # filter just for the 2021 season
MarJul2020 <- rec.ll %>% filter(date > "2020-03-01" & date < "2020-08-01") # just 2020

#### Compare buffer sizes ####
a_adj <- PointAdjust(MarJul2020) # adjust filtered points
b_adj <- PointAdjust(MarJul2021)
# 10 m (territory)
a_KD <- KD_extract(NCAplus_utm, a_adj, 1000, HolaNests_utm, 10)
b_KD <- KD_extract(NCAplus_utm, b_adj, 1000, HolaNests_utm, 10)
HOLArec$Buffer10m_2020 <- a_KD
HOLArec$Buffer10m_2021 <- b_KD

# 100 m (territory)
a_KD <- KD_extract(NCAplus_utm, a_adj, 1000, HolaNests_utm, 100)
b_KD <- KD_extract(NCAplus_utm, b_adj, 1000, HolaNests_utm, 100)
HOLArec$Buffer100m_2020 <- a_KD
HOLArec$Buffer100m_2021 <- b_KD

# 500 m 
a_KD <- KD_extract(NCAplus_utm, a_adj, 1000, HolaNests_utm, 500)
b_KD <- KD_extract(NCAplus_utm, b_adj, 1000, HolaNests_utm, 500)
HOLArec$Buffer500m_2020 <- a_KD
HOLArec$Buffer500m_2021 <- b_KD
# 1000 m
a_KD <- KD_extract(NCAplus_utm, a_adj, 1000, HolaNests_utm, 1000)
b_KD <- KD_extract(NCAplus_utm, b_adj, 1000, HolaNests_utm, 1000)
HOLArec$Buffer1000m_2020 <- a_KD
HOLArec$Buffer1000m_2021 <- b_KD

# look at values
kableExtra::kable(HOLArec)

write.csv(HOLArec, "HOLArec.csv") # this is the file that can be taken & used in HOLA analyses - saving in Driving Analysis folder
write.csv(HOLArec, file = "/Users/madelineaberg/Desktop/HOLA2020/2020analysis/HOLA_analysis/datatemp/original/HOLArec.csv") # save in HOLA analysis R project

```


